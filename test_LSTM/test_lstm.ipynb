{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, utils\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cloud</th>\n",
       "      <th>wind</th>\n",
       "      <th>ave_tmp</th>\n",
       "      <th>max_tmp</th>\n",
       "      <th>min_tmp</th>\n",
       "      <th>rain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-17</th>\n",
       "      <td>9.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.4</td>\n",
       "      <td>21.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-18</th>\n",
       "      <td>7.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.4</td>\n",
       "      <td>27.1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-19</th>\n",
       "      <td>7.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>21.5</td>\n",
       "      <td>26.3</td>\n",
       "      <td>15.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-20</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>16.8</td>\n",
       "      <td>22.1</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-21</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>20.3</td>\n",
       "      <td>27.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-13</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>18.1</td>\n",
       "      <td>15.4</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-14</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>14.3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-15</th>\n",
       "      <td>9.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>18.1</td>\n",
       "      <td>14.6</td>\n",
       "      <td>15.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-16</th>\n",
       "      <td>2.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>19.9</td>\n",
       "      <td>27.5</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>22.8</td>\n",
       "      <td>31.6</td>\n",
       "      <td>15.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1827 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            cloud  wind  ave_tmp  max_tmp  min_tmp  rain\n",
       "2018-05-17    9.0   3.7     24.0     28.4     21.4   0.0\n",
       "2018-05-18    7.8   3.0     23.4     27.1     20.0   0.0\n",
       "2018-05-19    7.5   3.9     21.5     26.3     15.3   0.0\n",
       "2018-05-20    4.3   3.5     16.8     22.1     13.1   0.0\n",
       "2018-05-21   10.0   2.4     20.3     27.2     13.7   0.0\n",
       "...           ...   ...      ...      ...      ...   ...\n",
       "2023-05-13   10.0   3.0     16.7     18.1     15.4   5.0\n",
       "2023-05-14   10.0   3.6     16.9     21.2     14.3   3.0\n",
       "2023-05-15    9.8   2.0     16.1     18.1     14.6  15.5\n",
       "2023-05-16    2.8   2.6     19.9     27.5     13.1   0.0\n",
       "2023-05-17    0.0   2.3     22.8     31.6     15.1   0.0\n",
       "\n",
       "[1827 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv(\"https://raw.githubusercontent.com/aweglteo/tokyo_weather_data/main/data.csv\", parse_dates=True, index_col=0)\n",
    "# df.to_csv(\"./data/wether.csv\")\n",
    "df = pd.read_csv(\"./data/wether.csv\", parse_dates=True, index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(r):\n",
    "    norm = np.linalg.norm(r)\n",
    "    return r / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1827 entries, 0 to 1826\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   cloud    1827 non-null   float64\n",
      " 1   wind     1827 non-null   float64\n",
      " 2   ave_tmp  1827 non-null   float64\n",
      " 3   max_tmp  1827 non-null   float64\n",
      " 4   min_tmp  1827 non-null   float64\n",
      " 5   rain     1827 non-null   float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 85.8 KB\n"
     ]
    }
   ],
   "source": [
    "cloud = normalize(df[\"cloud\"])\n",
    "wind = normalize(df[\"wind\"])\n",
    "ave_tmp = normalize(df[\"ave_tmp\"])\n",
    "max_tmp = normalize(df[\"max_tmp\"])\n",
    "min_tmp = normalize(df[\"min_tmp\"])\n",
    "rain = normalize(df[\"rain\"])\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    np.array([\n",
    "        cloud,\n",
    "        wind,\n",
    "        ave_tmp,\n",
    "        max_tmp,\n",
    "        min_tmp,\n",
    "        rain        \n",
    "    ]).T,\n",
    "    columns=[\n",
    "        \"cloud\",\n",
    "        \"wind\",\n",
    "        \"ave_tmp\",\n",
    "        \"max_tmp\",\n",
    "        \"min_tmp\",\n",
    "        \"rain\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(data, test_size=0.3, shuffle=False)\n",
    "ws, n_dim = 20, df_train.shape[1]\n",
    "n_train, n_test = len(df_train) - ws, len(df_test) - ws\n",
    "\n",
    "train = np.array([df_train.iloc[i:i+ws].values for i in range(n_train)])\n",
    "train_labels = np.array([df_train.iloc[i+ws].values for i in range(n_train)])[:, 2]\n",
    "test = np.array([df_test.iloc[i:i+ws].values for i in range(n_test)])\n",
    "test_labels = np.array([df_test.iloc[i+ws].values for i in range(n_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.tensor(train, dtype=torch.float)\n",
    "labels = torch.tensor(train_labels, dtype=torch.float)\n",
    "dataset = torch.utils.data.TensorDataset(train, labels)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_dim, n_layers):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        \n",
    "        self.feature_size = feature_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_output = 1\n",
    "        \n",
    "        self.lstm = nn.LSTM(feature_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, self.n_output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # hidden stateとcell stateをゼロで初期化\n",
    "        h_0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c_0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0), self.hidden_dim)\n",
    "        \n",
    "        hn = hn.view(-1, self.hidden_dim)\n",
    "        y = self.fc(hn).reshape(self.n_output, -1)\n",
    "        \n",
    "        return y\n",
    "\n",
    "feature_size = train\n",
    "n_hidden = 64\n",
    "n_layers = 1\n",
    "net = MyLSTM(feature_size, n_hidden, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MyLSTM                                   --\n",
       "├─LSTM: 1-1                              18,432\n",
       "├─Linear: 1-2                            65\n",
       "=================================================================\n",
       "Total params: 18,497\n",
       "Trainable params: 18,497\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyLSTM(\n",
       "  (lstm): LSTM(6, 64, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_loss = nn.MSELoss()\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.001)\n",
    "loss_history = []\n",
    "epochs = 200\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LSTM.forward() takes from 2 to 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 7\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m func_loss(y, t)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/PBL_stock-HqIXwvyC/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/PBL_stock-HqIXwvyC/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 18\u001b[0m, in \u001b[0;36mMyLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m h_0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     16\u001b[0m c_0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 18\u001b[0m output, (hn, cn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m hn \u001b[38;5;241m=\u001b[39m hn\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim)\n\u001b[1;32m     21\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(hn)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_output, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/PBL_stock-HqIXwvyC/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/PBL_stock-HqIXwvyC/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: LSTM.forward() takes from 2 to 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "for i in range(epochs+1):\n",
    "    net.train()\n",
    "    tmp_loss = 0.0\n",
    "    for j, (x, t) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y = net(x) \n",
    "        y = y.to('cpu')\n",
    "        loss = func_loss(y, t)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        tmp_loss += loss.item()\n",
    "    tmp_loss /= j+1\n",
    "    loss_history.append(tmp_loss)\n",
    "    print('Epoch:', i, 'Loss_Train:', tmp_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PBL_stock-HqIXwvyC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
