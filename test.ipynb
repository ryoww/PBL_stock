{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "from key import id, password\n",
    "\n",
    "prefs = {\n",
    "        \"download.default_directory\": \"C:\\\\Users\\\\MIRAI\\\\programs\\\\syllabus\\\\pdf\\\\\",\n",
    "        \"download.prompt_for_download\": False,\n",
    "        \"plugins.always_open_pdf_externally\": True,\n",
    "        }\n",
    "\n",
    "ID = id\n",
    "PASS = password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "\n",
    "options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "driver.get(\"https://webcit-5.edu.metro-cit.ac.jp/users/sign_in\")\n",
    "\n",
    "driver.find_element(By.NAME, \"user[name]\").send_keys(ID)\n",
    "driver.find_element(By.NAME, \"user[password]\").send_keys(PASS)\n",
    "\n",
    "driver.find_element(By.NAME, \"commit\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "\n",
    "driver.execute_script(\"window.open('https://webcit-5.edu.metro-cit.ac.jp/gnumbers/33494/last_hyoukas');\")\n",
    "\n",
    "# 現在のウィンドウハンドルを取得\n",
    "original_window = driver.current_window_handle\n",
    "\n",
    "# 新しいタブまたはウィンドウに切り替え\n",
    "new_window = [window for window in driver.window_handles if window != original_window][0]\n",
    "driver.switch_to.window(new_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = driver.find_elements(By.CSS_SELECTOR, \"table.bordered tr\")\n",
    "\n",
    "for row in rows:\n",
    "    cells = row.find_elements(By.CSS_SELECTOR, \"td\")\n",
    "    if cells:\n",
    "        last_cell = cells[-1]\n",
    "        if (last_cell.text != \"PDFファイルが作成させていません。\"):\n",
    "            # print(last_cell.text)\n",
    "            last_cell.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "def merge_pdfs(directory, output_filename):\n",
    "    # PdfWriter オブジェクトの初期化\n",
    "    writer = PdfWriter()\n",
    "\n",
    "    # 指定されたディレクトリ内のファイルをループする\n",
    "    for item in sorted(os.listdir(directory)):\n",
    "        if item.endswith('.pdf'):\n",
    "            # PDFファイルを開く\n",
    "            filepath = os.path.join(directory, item)\n",
    "            reader = PdfReader(filepath)\n",
    "\n",
    "            # 各ページを書き込む\n",
    "            for page in reader.pages:\n",
    "                writer.add_page(page)\n",
    "\n",
    "    # 結合したPDFを保存\n",
    "    with open(output_filename, 'wb') as f:\n",
    "        writer.write(f)\n",
    "\n",
    "# 使用例\n",
    "directory = './pdf'\n",
    "output_filename = './output.pdf'\n",
    "merge_pdfs(directory, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sbiにあるか確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from key import sbi_id, sbi_pass\n",
    "\n",
    "def remove_symbol_from_json(symbol):\n",
    "    with open('./scrape_data/stock_name.json', 'r', encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for category in data:\n",
    "        data[category] = [company for company in data[category] if company[\"symbol\"] != symbol]\n",
    "\n",
    "    with open('./scrape_data/stock_name.json', 'w', encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "caps = DesiredCapabilities.CHROME\n",
    "caps['goog:loggingPrefs'] = {'performance': 'ALL'}\n",
    "\n",
    "options = Options()\n",
    "options.add_experimental_option(\"detach\", True)\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, Gecko) Chrome/58.0.3029.110 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "driver.get(\"https://www.sbisec.co.jp/ETGate\")\n",
    "\n",
    "id_input = driver.find_element(By.NAME, \"user_id\")\n",
    "id_input.send_keys(sbi_id)\n",
    "\n",
    "pass_input = driver.find_element(By.NAME, \"user_password\")\n",
    "pass_input.send_keys(sbi_pass)\n",
    "\n",
    "driver.find_element(By.NAME, \"ACT_login\").click()\n",
    "\n",
    "print(\"Please input Enter\")\n",
    "input(\"\")\n",
    "\n",
    "driver.get(\"https://global.sbisec.co.jp/home\")\n",
    "time.sleep(0.3)\n",
    "\n",
    "driver.find_element(By.XPATH, r'//*[@id=\"root\"]/main/article/div[1]/div[2]/a[2]').click()\n",
    "\n",
    "# JSONファイルの読み込み\n",
    "with open('./scrape_data/stock_name.json', 'r', encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 全てのシンボルをプリント\n",
    "for category in data:\n",
    "    for company in data[category]:\n",
    "        stock_code = company[\"symbol\"]\n",
    "        stock_name = company[\"company_name\"]\n",
    "        print(stock_code, ':', stock_name)\n",
    "\n",
    "        try:\n",
    "            driver.get(f\"https://global.sbisec.co.jp/invest/us/stock/{stock_code}?resource=news&searchType=include\")\n",
    "            time.sleep(0.3)\n",
    "\n",
    "            iframe = driver.page_source\n",
    "            soup = BeautifulSoup(iframe, \"html.parser\")\n",
    "            url_pattern = \"https://graph.sbisec.co.jp/sbinews/pc?\"\n",
    "            iframe_urls = [iframe.get('src') for iframe in soup.find_all('iframe', src=True) if iframe['src'].startswith(url_pattern)]\n",
    "\n",
    "            token = iframe_urls[0].split(\"token=\")[1]\n",
    "            url_head = f\"https://graph.sbisec.co.jp/sbinews/srvdetail?symbol={stock_code}&token={token}\"\n",
    "            print(url_head)\n",
    "\n",
    "            res_head = requests.get(url_head)\n",
    "            data_head = res_head.json()\n",
    "\n",
    "            len_head = len(data_head['data'])\n",
    "            time.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {stock_code}: {e}\")\n",
    "            remove_symbol_from_json(stock_code)\n",
    "            print(f\"Removed {stock_code} from stock_name.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# stockData = json.load(open(\"./scrape_data/stock_name.json\", encoding=\"cp932\"))\n",
    "\n",
    "with open('./stock_name.json', 'r', encoding=\"utf-8\") as file:\n",
    "    stockData = json.load(file)\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100\",\n",
    "    \"Accept-Language\": \"ja,en;q=0.9,en-GB;q=0.8,en-US;q=0.7\",\n",
    "}\n",
    "\n",
    "for category in stockData:\n",
    "    for company in stockData[category]:\n",
    "        stock_code = company[\"symbol\"]\n",
    "        response = requests.get(\n",
    "            f\"https://api.nasdaq.com/api/quote/{stock_code}/info?assetclass=stocks\",\n",
    "            headers=headers,\n",
    "        )\n",
    "        fetchedData = response.json()\n",
    "        if fetchedData[\"data\"] == None:\n",
    "            print(\"アッ！！\", stock_code)\n",
    "            # break\n",
    "\n",
    "        print(\"ok\", stock_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.nasdaq.com/api/quote/NVDA/historical?assetclass=stocks&fromdate=2024-01-01&limit=9999&todate=2024-07-27&random=43'\n",
    "res = requests.get(url, headers=headers)\n",
    "\n",
    "data = res.json()\n",
    "print(data['data']['tradesTable']['rows'])\n",
    "\n",
    "date_close_list = [{\"date\": row[\"date\"], \"close\": row[\"close\"]} for row in data[\"data\"][\"tradesTable\"][\"rows\"]]\n",
    "\n",
    "dates = [row[\"date\"] for row in data[\"data\"][\"tradesTable\"][\"rows\"]]\n",
    "closes = [row[\"close\"] for row in data[\"data\"][\"tradesTable\"][\"rows\"]]\n",
    "\n",
    "print(date_close_list[0]['date'])\n",
    "print(\"Dates:\", dates)\n",
    "print(\"Closes:\", closes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "BASE_URL = 'http://192.168.1.222:8999'\n",
    "\n",
    "with open('./stock_name.json', 'r', encoding='utf-8') as file:\n",
    "    stockData = json.load(file)\n",
    "\n",
    "for category in stockData:\n",
    "    for company in stockData[category]:\n",
    "        stock_code = company['symbol']\n",
    "        url = f'{BASE_URL}/getdays/{stock_code}'\n",
    "        res = requests.get(url)\n",
    "        print(res)\n",
    "        df = pd.DataFrame(res.json())\n",
    "        df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
    "        date_time_array = df['datetime'].to_numpy()\n",
    "        print(df.info())\n",
    "        # print(date_time_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# values get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "stocks_data = {}\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "with open('./stock_name.json', 'r', encoding=\"utf-8\") as file:\n",
    "    stockData = json.load(file)\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100\",\n",
    "    \"Accept-Language\": \"ja,en;q=0.9,en-GB;q=0.8,en-US;q=0.7\",\n",
    "}\n",
    "\n",
    "for category in stockData:\n",
    "    for company in stockData[category]:\n",
    "        stock_code = company[\"symbol\"]\n",
    "\n",
    "        url = f'https://api.nasdaq.com/api/quote/{stock_code}/historical?assetclass=stocks&fromdate=2024-01-01&limit=9999&todate={today}'\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        print(stock_code)\n",
    "        print(response)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # レスポンスから必要なデータを抽出（例：response.json()から 'rows' 部分を抽出）\n",
    "            stock_data = response.json()[\"data\"][\"tradesTable\"][\"rows\"]\n",
    "            # 必要なフィールドのみ抽出して辞書に追加\n",
    "            stocks_data[stock_code] = [{\"date\": row[\"date\"], \"close\": row[\"close\"]} for row in stock_data]\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for {stock_code}\")\n",
    "\n",
    "with open('./stocks_values.json', 'w') as json_file:\n",
    "    json.dump(stocks_data, json_file, indent=4)\n",
    "\n",
    "print(\"JSONファイルに保存されました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from key import BASE_URL\n",
    "\n",
    "url = f'{BASE_URL}/null_values/ZTS'\n",
    "res = requests.get(url)\n",
    "data = res.json()\n",
    "\n",
    "for date, id_list in data.items():\n",
    "    print(date)\n",
    "    # for id in id_list:\n",
    "        # print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "BASE_URL = 'http://192.168.1.222:8999'\n",
    "post_url = f'{BASE_URL}/spot_update'\n",
    "\n",
    "def find_previous_value(stock_symbol, date_str, json_days, entries):\n",
    "    # 再帰的に1日ずつ前にさかのぼる\n",
    "    date_obj = datetime.strptime(date_str, \"%Y-%m-%d\") - timedelta(days=1)\n",
    "    previous_date_str = date_obj.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # エントリーに前日の日付が存在するか確認\n",
    "    for entry in entries:\n",
    "        if entry['date'] == previous_date_str:\n",
    "            return entry['close']  # 存在すればその値を返す\n",
    "\n",
    "    # 存在しなければさらに1日さかのぼる\n",
    "    return find_previous_value(stock_symbol, previous_date_str, json_days, entries)\n",
    "\n",
    "def remove_symbol_from_json(symbol):\n",
    "    with open('./stock_name.json', 'r', encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for category in data:\n",
    "        data[category] = [company for company in data[category] if company[\"symbol\"] != symbol]\n",
    "\n",
    "    with open('./stock_name.json', 'w', encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open('./stocks_values.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for stock_symbol, entries in reversed(data.items()):\n",
    "    print(f'Stock Symbol: {stock_symbol}')\n",
    "\n",
    "    days_url = f'{BASE_URL}/null_values/{stock_symbol}'\n",
    "    days = requests.get(days_url).json()\n",
    "\n",
    "    json_days = list(set(entry['date'] for entry in entries))\n",
    "\n",
    "    try:\n",
    "        for date, id_list in days.items():\n",
    "            if date not in json_days:\n",
    "                update_value = find_previous_value(stock_symbol, date, json_days, entries)\n",
    "                print(f'Missing date: {date}, filling with previous value: {update_value}')\n",
    "\n",
    "                for id in id_list:\n",
    "                    data = {\n",
    "                        'id' : id,\n",
    "                        'column_name' : 'value',\n",
    "                        'update_value' : update_value\n",
    "                    }\n",
    "                    # print(data)\n",
    "\n",
    "                    response = requests.post(post_url, json=data)\n",
    "                    print(response)\n",
    "            else:\n",
    "                for entry in entries:\n",
    "                    if entry['date'] == date:\n",
    "                        update_value = entry['close']\n",
    "                        print(f\"Date: {date}, Close: {update_value}\")\n",
    "\n",
    "                        for id in id_list:\n",
    "                            data = {\n",
    "                                'id' : id,\n",
    "                                'column_name' : 'value',\n",
    "                                'update_value' : update_value\n",
    "                            }\n",
    "                            # print(data)\n",
    "\n",
    "                            response = requests.post(post_url, json=data)\n",
    "                            print(response)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "        remove_symbol_from_json(stock_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "BASE_URL = 'http://192.168.1.222:8999'\n",
    "post_url = f'{BASE_URL}/spot_update'\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "values_data = {}\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100\",\n",
    "    \"Accept-Language\": \"ja,en;q=0.9,en-GB;q=0.8,en-US;q=0.7\",\n",
    "}\n",
    "\n",
    "with open('./stock_name.json', 'r', encoding=\"utf-8\") as file:\n",
    "    stockData = json.load(file)\n",
    "\n",
    "def convert_date(date_str):\n",
    "    date_obj = datetime.strptime(date_str, '%m/%d/%Y')\n",
    "\n",
    "    conved_date = date_obj.strftime('%Y-%m-%d')\n",
    "\n",
    "    return conved_date\n",
    "\n",
    "def remove_symbol_from_json(symbol):\n",
    "    with open('./stock_name.json', 'r', encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for category in data:\n",
    "        data[category] = [company for company in data[category] if company[\"symbol\"] != symbol]\n",
    "\n",
    "    with open('./stock_name.json', 'w', encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "for category in stockData:\n",
    "    for company in stockData[category]:\n",
    "        stock_code = company[\"symbol\"]\n",
    "\n",
    "        url = f'https://api.nasdaq.com/api/quote/{stock_code}/historical?assetclass=stocks&fromdate=2024-01-01&limit=9999&todate={today}'\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        print(stock_code)\n",
    "        print(response)\n",
    "        try:\n",
    "            if response.status_code == 200:\n",
    "                # レスポンスから必要なデータを抽出（例：response.json()から 'rows' 部分を抽出）\n",
    "                stock_data = response.json()[\"data\"][\"tradesTable\"][\"rows\"]\n",
    "                print(stock_data)\n",
    "                # 必要なフィールドのみ抽出して辞書に追加\n",
    "                values_data[stock_code] = [{\"date\": convert_date(row[\"date\"]), \"close\": row[\"close\"].replace('$', '')} for row in stock_data]\n",
    "            else:\n",
    "                print(f\"Failed to retrieve data for {stock_code}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            remove_symbol_from_json(stock_code)\n",
    "            continue\n",
    "\n",
    "with open('./stocks_values.json', 'w') as json_file:\n",
    "    json.dump(values_data, json_file, indent=4)\n",
    "\n",
    "print(\"saved stocks_values.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syllabus-yNspLzFI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
