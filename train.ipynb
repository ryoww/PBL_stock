{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryo/.local/share/virtualenvs/PBL_stock-HqIXwvyC/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification, BertConfig\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully : ./train_data/train.csv\n",
      "Succesfully : ./train_data/test.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def make_csv(path):\n",
    "    # JSONファイルを読み込む\n",
    "    with open('./train_data/'+path+'.json', 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # データをDataFrameに変換\n",
    "    columns = ['text', 'despair', 'optimism', 'concern', 'excitement', 'stability']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    for item in data:\n",
    "        row = [item['text']] + item['labels']\n",
    "        df.loc[len(df)] = row\n",
    "\n",
    "    # CSVファイルに保存\n",
    "    df.to_csv('./train_data/'+path+'.csv', index=False)\n",
    "\n",
    "    print(\"Succesfully :\", './train_data/'+path+'.csv')\n",
    "\n",
    "\n",
    "make_csv('train')\n",
    "make_csv('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>despair</th>\n",
       "      <th>optimism</th>\n",
       "      <th>concern</th>\n",
       "      <th>excitement</th>\n",
       "      <th>stability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1416 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      despair  optimism  concern  excitement  stability\n",
       "0         0.7       0.2      0.8         0.1        0.2\n",
       "1         0.1       0.9      0.2         0.8        0.7\n",
       "2         0.2       0.6      0.6         0.9        0.5\n",
       "3         0.0       0.9      0.1         0.7        0.8\n",
       "4         0.3       0.7      0.7         0.6        0.4\n",
       "...       ...       ...      ...         ...        ...\n",
       "1411      0.2       0.8      0.4         0.7        0.7\n",
       "1412      0.3       0.8      0.4         0.7        0.6\n",
       "1413      0.3       0.8      0.4         0.7        0.6\n",
       "1414      0.9       0.1      0.8         0.1        0.2\n",
       "1415      0.3       0.8      0.4         0.7        0.6\n",
       "\n",
       "[1416 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./train_data/train.csv\")\n",
    "\n",
    "text_train = df_train.text.values\n",
    "labels_train = df_train[df_train.columns[1:]]\n",
    "labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>despair</th>\n",
       "      <th>optimism</th>\n",
       "      <th>concern</th>\n",
       "      <th>excitement</th>\n",
       "      <th>stability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     despair  optimism  concern  excitement  stability\n",
       "0        0.2       0.8      0.4         0.7        0.6\n",
       "1        0.3       0.7      0.5         0.6        0.6\n",
       "2        0.4       0.6      0.7         0.5        0.6\n",
       "3        0.2       0.8      0.3         0.7        0.6\n",
       "4        0.3       0.7      0.5         0.6        0.6\n",
       "..       ...       ...      ...         ...        ...\n",
       "125      0.3       0.7      0.4         0.6        0.7\n",
       "126      0.2       0.8      0.3         0.7        0.7\n",
       "127      0.3       0.7      0.5         0.6        0.6\n",
       "128      0.2       0.8      0.3         0.7        0.7\n",
       "129      0.2       0.8      0.4         0.6        0.7\n",
       "\n",
       "[130 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('./train_data/test.csv')\n",
    "\n",
    "text_test = df_test.text.values\n",
    "labels_test = df_test[df_test.columns[1:]]\n",
    "labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ソニーグループ(6758.T)の株価が、期待を下回る四半期利益報告後に10%下落。市場の不安定さを反映し、業績見通しの下方修正が発表された。年内の売上予測が前年比で5%減少すると予想されている。\n",
      "['ソニー', 'グループ', '(', '67', '##5', '##8', '.', 'T', ')', 'の', '株価', 'が', '、', '期待', 'を', '下回る', '四', '##半期', '利益', '報告', '後', 'に', '10', '%', '下落', '。', '市場', 'の', '不安定', 'さ', 'を', '反映', 'し', '、', '業績', '見通し', 'の', '下方', '修正', 'が', '発表', 'さ', 'れ', 'た', '。', '年内', 'の', '売上', '予測', 'が', '前年', '比', 'で', '5', '%', '減少', 'する', 'と', '予想', 'さ', 'れ', 'て', 'いる', '。']\n",
      "[6369, 1091, 23, 5815, 28498, 28501, 143, 260, 24, 5, 17059, 14, 6, 3252, 11, 19997, 755, 17555, 5161, 1888, 83, 7, 121, 648, 18369, 8, 2304, 5, 8499, 26, 11, 6256, 15, 6, 6624, 15887, 5, 20659, 4971, 14, 602, 26, 20, 10, 8, 26334, 5, 6446, 7055, 14, 3065, 701, 12, 76, 648, 2643, 34, 13, 4663, 26, 20, 16, 33, 8]\n"
     ]
    }
   ],
   "source": [
    "print(text_train[0])\n",
    "print(tokenizer.tokenize(text_train[0]))\n",
    "print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_train[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max train len :  186\n",
      "max test len :  192\n",
      "194\n"
     ]
    }
   ],
   "source": [
    "print('max train len : ', max(len(tokenizer.tokenize(t)) for t in text_train))\n",
    "print('max test len : ', max(len(tokenizer.tokenize(t)) for t in text_test))\n",
    "max_len = int(max(max(len(tokenizer.tokenize(t)) for t in text_test), max(len(tokenizer.tokenize(t)) for t in text_train))) + 2\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ソニーグループ(6758.T)の株価が、期待を下回る四半期利益報告後に10%下落。市場の不安定さを反映し、業績見通しの下方修正が発表された。年内の売上予測が前年比で5%減少すると予想されている。\n",
      "tensor([    2,  6369,  1091,    23,  5815, 28498, 28501,   143,   260,    24,\n",
      "            5, 17059,    14,     6,  3252,    11, 19997,   755, 17555,  5161,\n",
      "         1888,    83,     7,   121,   648, 18369,     8,  2304,     5,  8499,\n",
      "           26,    11,  6256,    15,     6,  6624, 15887,     5, 20659,  4971,\n",
      "           14,   602,    26,    20,    10,     8, 26334,     5,  6446,  7055,\n",
      "           14,  3065,   701,    12,    76,   648,  2643,    34,    13,  4663,\n",
      "           26,    20,    16,    33,     8,     3,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n",
      "tensor([0.7000, 0.2000, 0.8000, 0.1000, 0.2000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_ids_train = []\n",
    "attention_masks_train = []\n",
    "input_ids_test = []\n",
    "attention_masks_test = []\n",
    "max_len = int(max(max(len(tokenizer.tokenize(t)) for t in text_test), max(len(tokenizer.tokenize(t)) for t in text_train))) + 2\n",
    "\n",
    "def mask_ids(text, input_ids, attention_masks):\n",
    "    for t in text:\n",
    "        encoded_dict = tokenizer(\n",
    "            t,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        \n",
    "mask_ids(text_train, input_ids_train, attention_masks_train)\n",
    "mask_ids(text_test, input_ids_test, attention_masks_test)\n",
    "\n",
    "# リストのTensorを連結\n",
    "input_ids_train = torch.cat(input_ids_train, dim=0)\n",
    "input_ids_test = torch.cat(input_ids_test, dim=0)\n",
    "\n",
    "attention_masks_train = torch.cat(attention_masks_train, dim=0)\n",
    "attention_masks_test = torch.cat(attention_masks_test, dim=0)\n",
    "\n",
    "labels_train = torch.tensor(np.array(labels_train))\n",
    "labels_test = torch.tensor(np.array(labels_test))\n",
    "\n",
    "\n",
    "# 最初のテキストとその入力IDを印刷\n",
    "print(text_train[0])\n",
    "print(input_ids_train[0])\n",
    "print(labels_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 1274\n",
      "valid size  142\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "test_dataset = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "\n",
    "\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "valid_size = len(train_dataset) - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(train_dataset, [train_size, valid_size])\n",
    "\n",
    "print('train size', train_size)\n",
    "print('valid size ', valid_size)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    sampler=SequentialSampler(valid_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
    "    num_labels = 5,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False\n",
    ")\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, dataloader, device):\n",
    "    model.eval()  # モデルを評価モードに設定\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # モデルからロジットを取得\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # ソフトマックスを適用して確率を計算\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            predicted_labels = torch.argmax(probabilities, dim=1)\n",
    "            true_labels = torch.argmax(b_labels, dim=1)\n",
    "\n",
    "            # 正確な予測の数をカウント\n",
    "            correct_predictions += (predicted_labels == true_labels).sum().item()\n",
    "            total_predictions += b_labels.size(0)\n",
    "\n",
    "    # 精度を計算\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "def save_model(model, optimizer, accuracy, file_path=\"./model/\"):\n",
    "    accuracy = f\"{accuracy:.4f}\".replace('.', '_')\n",
    "    full_path = f\"{file_path}bert_{accuracy}.pt\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, full_path)\n",
    "    print(f\"Model saved to {full_path}\")\n",
    "    \n",
    "\n",
    "def load_model(model, optimizer, file_path):\n",
    "    checkpoint = torch.load(file_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print(f\"Model loaded from {file_path}\")\n",
    "\n",
    "def train(model, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m test_loss_ \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epoch):\n\u001b[0;32m----> 6\u001b[0m     train_ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     train_loss_\u001b[38;5;241m.\u001b[39mappend(train_)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m max_epoch \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# Last epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer)\u001b[0m\n\u001b[1;32m     38\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     39\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 40\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "max_epoch = 10\n",
    "train_loss_ = []\n",
    "test_loss_ = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    train_ = train(model, optimizer)\n",
    "    train_loss_.append(train_)\n",
    "    \n",
    "    if epoch == max_epoch - 1:  # Last epoch\n",
    "        valid_accuracy = calculate_accuracy(model, valid_dataloader, device)\n",
    "        save_model(model, optimizer, valid_accuracy)  # Save model with train accuracy in filename\n",
    "\n",
    "accuracy = f\"{valid_accuracy:.4f}\".replace('.', '_')\n",
    "load_model(model, optimizer, f'./model/bert_{accuracy}.pt')  # Load the model before testing\n",
    "test_ = test(model)\n",
    "test_loss_.append(test_)\n",
    "\n",
    "\n",
    "# Calculate accuracy after loading the model\n",
    "train_accuracy = calculate_accuracy(model, train_dataloader, device)\n",
    "test_accuracy = calculate_accuracy(model, test_dataloader, device)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./model/bert_0_9567.pt\n",
      "[0.38150236 0.05312215 0.38559145 0.05664619 0.12313791]\n"
     ]
    }
   ],
   "source": [
    "def predict_text_probabilities(model, tokenizer, text, device):\n",
    "    # テキストをトークナイズしてテンソルに変換\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # モデルの評価モード\n",
    "    model.eval()\n",
    "\n",
    "    # 予測の実行\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    # 確率をnumpy配列に変換して返す\n",
    "    return probabilities.squeeze().cpu().numpy()\n",
    "\n",
    "modelname_1 = './model/bert_0_9261.pt'\n",
    "modelname_2 = './model/bert_0_9391.pt'\n",
    "\n",
    "modelname = './model/bert_0_9567.pt'\n",
    "\n",
    "\n",
    "# このモデルかなりい良い\n",
    "load_model(model, optimizer, modelname)\n",
    "\n",
    "# 使用例 ['text', 'despair', 'optimism', 'concern', 'excitement', 'stability'] 失望、楽観、懸念、興奮、安定\n",
    "text_to_classify = \"彼のプレゼンテーションは期待外れだった。\"\n",
    "probabilities = predict_text_probabilities(model, tokenizer, text_to_classify, device)\n",
    "print(probabilities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36090022 0.06583396 0.42395124 0.0684452  0.08086935]\n"
     ]
    }
   ],
   "source": [
    "text = '［ドバイ　２０日　ロイター］ -     イラン当局者は２０日、ライシ大統領とアブドラヒアン外相が、搭乗していたヘリコプター墜落で死亡したとロイターに述べた。'\n",
    "\n",
    "predict = predict_text_probabilities(model, tokenizer, text, device)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04141054 0.4326825  0.06948418 0.22149023 0.23493247]\n"
     ]
    }
   ],
   "source": [
    "text = '【速報】世界が注目するNVIDIA(エヌビディア)が決算発表「最終的な利益 前年比7.3倍2兆3300億円」勢い止まらず'\n",
    "\n",
    "predict = predict_text_probabilities(model, tokenizer, text, device)\n",
    "print(predict)\n",
    "\n",
    "# ['text', 'despair', 'optimism', 'concern', 'excitement', 'stability']\n",
    "# 失望、楽観、懸念、興奮、安定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PBL_stock-HqIXwvyC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
