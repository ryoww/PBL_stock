{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification, BertConfig\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully : ./train_data/test.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# JSONファイルのパス\n",
    "json_file_path = './train_data/test.json'  # JSONファイルのパスをここに設定してください\n",
    "\n",
    "# CSVファイルの出力パス\n",
    "csv_file_path = './train_data/test.csv'\n",
    "\n",
    "# JSONファイルを読み込む\n",
    "with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# データをDataFrameに変換\n",
    "columns = ['text', 'despair', 'optimism', 'concern', 'excitement', 'stability']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for item in data:\n",
    "    row = [item['text']] + item['labels']\n",
    "    df.loc[len(df)] = row\n",
    "\n",
    "# CSVファイルに保存\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(\"Succesfully :\", csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>despair</th>\n",
       "      <th>optimism</th>\n",
       "      <th>concern</th>\n",
       "      <th>excitement</th>\n",
       "      <th>stability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1023 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      despair  optimism  concern  excitement  stability\n",
       "0         0.7       0.2      0.8         0.1        0.2\n",
       "1         0.1       0.9      0.2         0.8        0.7\n",
       "2         0.2       0.6      0.6         0.9        0.5\n",
       "3         0.0       0.9      0.1         0.7        0.8\n",
       "4         0.3       0.7      0.7         0.6        0.4\n",
       "...       ...       ...      ...         ...        ...\n",
       "1018      0.8       0.1      0.9         0.1        0.4\n",
       "1019      0.8       0.1      0.9         0.1        0.4\n",
       "1020      0.1       0.7      0.1         0.4        0.6\n",
       "1021      0.7       0.1      0.8         0.1        0.3\n",
       "1022      0.8       0.1      0.9         0.1        0.4\n",
       "\n",
       "[1023 rows x 5 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./train_data/test.csv\")\n",
    "\n",
    "text = df.text.values\n",
    "labels = df[df.columns[1:]]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ソニーグループ(6758.T)の株価が、期待を下回る四半期利益報告後に10%下落。市場の不安定さを反映し、業績見通しの下方修正が発表された。年内の売上予測が前年比で5%減少すると予想されている。\n",
      "['ソニー', 'グループ', '(', '67', '##5', '##8', '.', 'T', ')', 'の', '株価', 'が', '、', '期待', 'を', '下回る', '四', '##半期', '利益', '報告', '後', 'に', '10', '%', '下落', '。', '市場', 'の', '不安定', 'さ', 'を', '反映', 'し', '、', '業績', '見通し', 'の', '下方', '修正', 'が', '発表', 'さ', 'れ', 'た', '。', '年内', 'の', '売上', '予測', 'が', '前年', '比', 'で', '5', '%', '減少', 'する', 'と', '予想', 'さ', 'れ', 'て', 'いる', '。']\n",
      "[6369, 1091, 23, 5815, 28498, 28501, 143, 260, 24, 5, 17059, 14, 6, 3252, 11, 19997, 755, 17555, 5161, 1888, 83, 7, 121, 648, 18369, 8, 2304, 5, 8499, 26, 11, 6256, 15, 6, 6624, 15887, 5, 20659, 4971, 14, 602, 26, 20, 10, 8, 26334, 5, 6446, 7055, 14, 3065, 701, 12, 76, 648, 2643, 34, 13, 4663, 26, 20, 16, 33, 8]\n"
     ]
    }
   ],
   "source": [
    "print(text[0])\n",
    "print(tokenizer.tokenize(text[0]))\n",
    "print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len :  186\n"
     ]
    }
   ],
   "source": [
    "max_len = []\n",
    "\n",
    "for t in text:\n",
    "    words = tokenizer.tokenize(t)\n",
    "    max_len.append(len(words))\n",
    "\n",
    "print('max len : ', max(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ソニーグループ(6758.T)の株価が、期待を下回る四半期利益報告後に10%下落。市場の不安定さを反映し、業績見通しの下方修正が発表された。年内の売上予測が前年比で5%減少すると予想されている。\n",
      "tensor([    2,  6369,  1091,    23,  5815, 28498, 28501,   143,   260,    24,\n",
      "            5, 17059,    14,     6,  3252,    11, 19997,   755, 17555,  5161,\n",
      "         1888,    83,     7,   121,   648, 18369,     8,  2304,     5,  8499,\n",
      "           26,    11,  6256,    15,     6,  6624, 15887,     5, 20659,  4971,\n",
      "           14,   602,    26,    20,    10,     8, 26334,     5,  6446,  7055,\n",
      "           14,  3065,   701,    12,    76,   648,  2643,    34,    13,  4663,\n",
      "           26,    20,    16,    33,     8,     3,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "tensor([0.7000, 0.2000, 0.8000, 0.1000, 0.2000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "max_len = int(max(max_len) + 2)\n",
    "\n",
    "for t in text:\n",
    "    encoded_dict = tokenizer(\n",
    "        t,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# リストのTensorを連結\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "labels = torch.tensor(np.array(labels))\n",
    "\n",
    "# 最初のテキストとその入力IDを印刷\n",
    "print(text[0])\n",
    "print(input_ids[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 920\n",
      "test size  103\n"
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print('train size', train_size)\n",
    "print('test size ', test_size)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    sampler=SequentialSampler(test_dataset),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
    "    num_labels = 5,\n",
    "    output_attentions = True,\n",
    "    output_hidden_states = True\n",
    ")\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, dataloader, device):\n",
    "    model.eval()  # モデルを評価モードに設定\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # モデルからロジットを取得\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # ソフトマックスを適用して確率を計算\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            predicted_labels = torch.argmax(probabilities, dim=1)\n",
    "            true_labels = torch.argmax(b_labels, dim=1)\n",
    "\n",
    "            # 正確な予測の数をカウント\n",
    "            correct_predictions += (predicted_labels == true_labels).sum().item()\n",
    "            total_predictions += b_labels.size(0)\n",
    "\n",
    "    # 精度を計算\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "def save_model(model, optimizer, accuracy, file_path=\"./model/\"):\n",
    "    accuracy = f\"{accuracy:.4f}\".replace('.', '_')\n",
    "    full_path = f\"{file_path}bert_{accuracy}.pt\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, full_path)\n",
    "    print(f\"Model saved to {full_path}\")\n",
    "    \n",
    "\n",
    "def load_model(model, optimizer, file_path):\n",
    "    checkpoint = torch.load(file_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print(f\"Model loaded from {file_path}\")\n",
    "\n",
    "def train(model, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./model/bert_0_8283.pt\n",
      "Model loaded from ./model/bert_0_8283.pt\n",
      "Training Accuracy: 0.8283\n",
      "Test Accuracy: 0.8077\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_epoch = 100\n",
    "train_loss_ = []\n",
    "test_loss_ = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    train_ = train(model, optimizer)\n",
    "    train_loss_.append(train_)\n",
    "    \n",
    "    if epoch == max_epoch - 1:  # Last epoch\n",
    "        train_accuracy = calculate_accuracy(model, train_dataloader, device)\n",
    "        save_model(model, optimizer, train_accuracy)  # Save model with train accuracy in filename\n",
    "\n",
    "accuracy = f\"{train_accuracy:.4f}\".replace('.', '_')\n",
    "load_model(model, optimizer, f'./model/bert_{accuracy}.pt')  # Load the model before testing\n",
    "test_ = test(model)\n",
    "test_loss_.append(test_)\n",
    "\n",
    "\n",
    "# Calculate accuracy after loading the model\n",
    "train_accuracy = calculate_accuracy(model, train_dataloader, device)\n",
    "test_accuracy = calculate_accuracy(model, test_dataloader, device)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from /home/ryo/PBL_stock/model/bert_0_9261.pt\n",
      "[0.40331084 0.06602882 0.37462568 0.05223165 0.10380296]\n"
     ]
    }
   ],
   "source": [
    "def predict_text_probabilities(model, tokenizer, text, device):\n",
    "    # テキストをトークナイズしてテンソルに変換\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # モデルの評価モード\n",
    "    model.eval()\n",
    "\n",
    "    # 予測の実行\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    # 確率をnumpy配列に変換して返す\n",
    "    return probabilities.squeeze().cpu().numpy()\n",
    "\n",
    "modelname_1 = './bert_model/bert_0_9261.pt'\n",
    "modelname_2 = './bert_model/bert_0_9391.pt'\n",
    "\n",
    "\n",
    "# このモデルかなりい良い\n",
    "load_model(model, optimizer, modelname_1)\n",
    "\n",
    "# 使用例 ['text', 'despair', 'optimism', 'concern', 'excitement', 'stability'] 失望、楽観、懸念、興奮、安定\n",
    "text_to_classify = \"彼のプレゼンテーションは期待外れだった。\"\n",
    "probabilities = predict_text_probabilities(model, tokenizer, text_to_classify, device)\n",
    "print(probabilities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0187923  0.36150253 0.02963696 0.30782    0.2822482 ]\n"
     ]
    }
   ],
   "source": [
    "text = 'このプロジェクトの成功に自信があります。'\n",
    "\n",
    "predict = predict_text_probabilities(model, tokenizer, text, device)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5651589  0.02576317 0.3479233  0.01399931 0.04715527]\n"
     ]
    }
   ],
   "source": [
    "text = 'サンシャインエネルギー社は今四半期の利益報告が市場の予想を下回り、株価は市場開始前に10%の下落を見せました。主に再生可能エネルギー源の開発遅延が原因で、会社は今後の見通しを慎重に修正し、投資家への影響を最小限に抑えるための対策を講じています。'\n",
    "\n",
    "predict = predict_text_probabilities(model, tokenizer, text, device)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PBL_stock-HqIXwvyC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
