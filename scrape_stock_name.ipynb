{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "\n",
    "options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\")\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "driver.get(\"https://www.180.co.jp/world_etf_adr/s&p500/a.htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for a...\n",
      "Processing data for b...\n",
      "Processing data for c...\n",
      "Processing data for d...\n",
      "Processing data for e...\n",
      "Processing data for f...\n",
      "Processing data for g...\n",
      "Processing data for h...\n",
      "Processing data for i...\n",
      "Processing data for j...\n",
      "Processing data for k...\n",
      "Processing data for l...\n",
      "Processing data for m...\n",
      "Processing data for n...\n",
      "Processing data for o...\n",
      "Processing data for p...\n",
      "Processing data for q...\n",
      "Processing data for r...\n",
      "Processing data for s...\n",
      "Processing data for t...\n",
      "Processing data for u...\n",
      "Processing data for v...\n",
      "Processing data for w...\n",
      "Processing data for x...\n",
      "Processing data for y...\n",
      "Processing data for z...\n",
      "All data has been saved to 'combined_data.json'\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def req(s):\n",
    "    # Construct the URL for the given suffix 's'\n",
    "    url = f\"https://www.180.co.jp/world_etf_adr/s&p500/{s}.htm\"\n",
    "\n",
    "    # Fetch the HTML content from the URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure the request was successful\n",
    "    response.encoding = 'utf-8'  # Set encoding to UTF-8\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all 'tr' tags\n",
    "    rows = soup.find_all('tr')\n",
    "\n",
    "    # Extract data from each row\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) == 3:\n",
    "            a_tag = cells[0].find('a')\n",
    "            symbol = a_tag.text.strip() if a_tag else \"N/A\"\n",
    "            company_name = cells[1].text.strip().replace('\\n', ' ').replace('  ', '')\n",
    "            description = cells[2].text.strip().replace('\\n', ' ').replace('  ', '')\n",
    "            data.append({\n",
    "                \"symbol\": symbol,\n",
    "                \"company_name\": company_name,\n",
    "                \"description\": description\n",
    "            })\n",
    "\n",
    "    # Return the extracted data\n",
    "    return data\n",
    "\n",
    "# Create a dictionary to hold all the data\n",
    "all_data = {}\n",
    "\n",
    "# Loop through each letter from 'a' to 'z'\n",
    "for letter in range(ord('a'), ord('z') + 1):\n",
    "    char = chr(letter)\n",
    "    print(f\"Processing data for {char}...\")\n",
    "    result = req(char)\n",
    "    all_data[char] = result  # Store the results under the key for each letter\n",
    "\n",
    "# Save all results to a single JSON file\n",
    "with open(\"./scrape_data/stock_name.json\", \"w\", encoding='utf-8') as json_file:\n",
    "    json.dump(all_data, json_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"All data has been saved to 'combined_data.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PBL_stock-c7g-vUE4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
